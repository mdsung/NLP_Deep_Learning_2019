{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    doc = doc.rstrip()\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation + '\\n')\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    tokens = ' '.join(tokens.split())\n",
    "    return tokens   \n",
    "    \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, label):\n",
    "    df = pd.DataFrame(columns=('text', 'label', 'wordcount'))\n",
    "    # walk through all files in the folder\n",
    "    x = 0\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        wordcount = len(tokens.split())\n",
    "        df = df.append({'text': tokens, 'label': label, 'wordcount': wordcount}, ignore_index=True)  \n",
    "    return df\n",
    "\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r', encoding=\"utf8\")\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    wordList = []\n",
    "    wordVectors = []    \n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        wordList.append(parts[0])\n",
    "        wordVectors.append(asarray(parts[1:], dtype='float32'))\n",
    "    return wordList, asarray(wordVectors, dtype='float32')\n",
    "\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 300))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding from file\n",
    "wordList, wordVectors = load_embedding('data/movie_embeddings_word2vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'movie', 'like', 'even', 'good', 'time', 'story', 'films', 'would']\n",
      "(44276, 300)\n",
      "<class 'numpy.ndarray'>\n",
      "[[-1.6218376e-01  5.5648273e-01 -2.7849936e-01 ...  1.8042541e-01\n",
      "  -2.0689143e-01 -5.5061924e-01]\n",
      " [-2.1016115e-01  3.1511781e-01 -3.4155941e-01 ...  2.1078728e-01\n",
      "  -2.4634373e-01 -3.4251076e-01]\n",
      " [-1.4809293e-01  6.1345339e-01 -1.4910689e-01 ...  2.7480820e-01\n",
      "  -2.3152344e-01 -5.6512904e-01]\n",
      " ...\n",
      " [-2.8129460e-03  2.0595028e-03 -2.8118331e-03 ...  3.2124331e-03\n",
      "  -3.4316683e-03 -7.1255555e-03]\n",
      " [-1.6010485e-03  1.7018503e-03 -9.0302055e-04 ...  1.6910424e-04\n",
      "  -6.4556394e-04 -1.4916587e-03]\n",
      " [-3.6617904e-03  2.4671869e-03 -4.3901941e-03 ...  8.2240645e-03\n",
      "  -3.9786389e-03 -1.1202857e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(wordList[1:10])\n",
    "print(wordVectors.shape)\n",
    "print(type(wordVectors))\n",
    "print(wordVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'data/movie_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "positive_docs = process_docs('data/txt_sentoken/pos', vocab, 1)\n",
    "negative_docs = process_docs('data/txt_sentoken/neg', vocab, 0)\n",
    "corpus = pd.concat([positive_docs, negative_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label wordcount\n",
      "0  films adapted comic books plenty success wheth...     1       402\n",
      "1  every movie comes along suspect studio every i...     1       356\n",
      "2  youve got mail works alot better deserves orde...     1       211\n",
      "3  jaws rare film grabs attention shows single im...     1       561\n",
      "4  moviemaking lot like general manager nfl team ...     1       371\n",
      "                                                  text label wordcount\n",
      "995  anything stigmata taken warning releasing simi...     0       668\n",
      "996  john boormans goofy cinematic debacle fundamen...     0       504\n",
      "997  kids hall acquired taste took least season wat...     0       200\n",
      "998  time john carpenter great horror director cour...     0       267\n",
      "999  two party guys bob heads haddaways dance hit l...     0       273\n",
      "\n",
      "Total number of cases = 2000\n",
      "label\n",
      "0    1000\n",
      "1    1000\n",
      "Name: label, dtype: int64\n",
      "Average number of words in a file = 350.0125\n"
     ]
    }
   ],
   "source": [
    "print(corpus.head())\n",
    "print(corpus.tail())\n",
    "print()\n",
    "print('Total number of cases =', corpus['text'].count())\n",
    "print(corpus.groupby('label').label.count())\n",
    "print('Average number of words in a file =', corpus.wordcount.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(corpus['wordcount'].tolist(), 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1500, 0, 200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 600\n",
    "numFiles = corpus['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "positiveCases = corpus.loc[corpus['label'] == 1]\n",
    "negativeCases = corpus.loc[corpus['label'] == 0]\n",
    "caseCounter = 0\n",
    "for index, row in positiveCases.iterrows():\n",
    "    indexCounter = 0\n",
    "    line=row['text']\n",
    "    split = line.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            ids[caseCounter][indexCounter] = wordList.index(word)\n",
    "        except ValueError:\n",
    "            ids[caseCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "            print(word)\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    caseCounter = caseCounter + 1 \n",
    "\n",
    "for index, row in negativeCases.iterrows():\n",
    "    indexCounter = 0\n",
    "    line=row['text']\n",
    "    split = line.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            ids[caseCounter][indexCounter] = wordList.index(word)\n",
    "        except ValueError:\n",
    "            ids[caseCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "            print(word)\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    caseCounter = caseCounter + 1 \n",
    "#np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Cases:  1000\n",
      "Negative Cases:  1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>films adapted comic books plenty success wheth...</td>\n",
       "      <td>1</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>every movie comes along suspect studio every i...</td>\n",
       "      <td>1</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youve got mail works alot better deserves orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaws rare film grabs attention shows single im...</td>\n",
       "      <td>1</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moviemaking lot like general manager nfl team ...</td>\n",
       "      <td>1</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label wordcount\n",
       "0  films adapted comic books plenty success wheth...     1       402\n",
       "1  every movie comes along suspect studio every i...     1       356\n",
       "2  youve got mail works alot better deserves orde...     1       211\n",
       "3  jaws rare film grabs attention shows single im...     1       561\n",
       "4  moviemaking lot like general manager nfl team ...     1       371"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Positive Cases: \", positiveCases.label.count())\n",
    "print(\"Negative Cases: \", negativeCases.label.count())\n",
    "positiveCases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,800)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(1200,2000)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0):\n",
    "            num = randint(800,1000)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(1000,1200)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 10001\n",
    "numDimensions = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create placeholders for the input data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a variable that will hold the input data in 3-D (batchsize, sequence length, word embedding dimension). Also map word indexes to word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tf.nn.rnn_cell.BasicLSTMCell function takes in an integer for the number of LSTM units. Tune this hyperparameter! Wrap that LSTM cell in a dropout layer to help prevent the network from overfitting. The tf.nn.dynamic_rnn function will unroll the whole network and create a pathway for the data to flow through the RNN graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=keep_prob)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-773132a4e1b5>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/pretrained_lstm.ckpt-500\n",
      "Accuracy:  50.0\n",
      "saved to models/pretrained_lstm.ckpt-1000\n",
      "Accuracy:  50.0\n",
      "saved to models/pretrained_lstm.ckpt-1500\n",
      "Accuracy:  49.833334028720856\n",
      "saved to models/pretrained_lstm.ckpt-2000\n",
      "Accuracy:  50.58333399891853\n",
      "saved to models/pretrained_lstm.ckpt-2500\n",
      "Accuracy:  49.41666725277901\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver(max_to_keep=10000)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels = getTrainBatch();\n",
    "   sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels, keep_prob: 0.75})\n",
    "\n",
    "   #Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "       summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels, keep_prob: 0.75})\n",
    "       writer.add_summary(summary, i)\n",
    "\n",
    "             #Save the network every 10,000 training iterations\n",
    "   if (i % 500 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "       acc = []\n",
    "       for j in range(100):\n",
    "            testBatch, testBatchLabels = getTestBatch()\n",
    "            batch_accuracy = sess.run(accuracy, {input_data: testBatch, labels: testBatchLabels, keep_prob: 1}) * 100\n",
    "            acc.append(batch_accuracy)\n",
    "       print(\"Accuracy: \", np.mean(acc)) \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, 'models/pretrained_lstm.ckpt-2000')\n",
    "#saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "acc = []\n",
    "truth = []\n",
    "pred = []\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    z = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels, keep_prob: 1.0}) * 100\n",
    "    p = sess.run(tf.argmax(prediction,1), {input_data: nextBatch, labels: nextBatchLabels, keep_prob: 1.0}) * 100\n",
    "    truth.extend((np.argmax(nextBatchLabels, 1) * 100))\n",
    "    pred.extend(p)\n",
    "    #    print(\"Accuracy for this batch:\", z)\n",
    "    acc.append(z)\n",
    "#    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = []\n",
    "#print(truth)\n",
    "#print(pred)\n",
    "cont.append([np.array(truth), np.array(pred)])\n",
    "best = cont[0]\n",
    "rf_ct = pd.crosstab(best[1], best[0], margins=True)\n",
    "rf_ct.columns = [\"-'ve\", \"+'ve\", \"Total\"]\n",
    "rf_ct.index = [\"-'ve\", \"+'ve\", \"Total\"]\n",
    "print(rf_ct)\n",
    "\n",
    "Sens = rf_ct.iloc[1][1]/rf_ct.iloc[2][1]\n",
    "Spec = rf_ct.iloc[0][0]/rf_ct.iloc[2][0]\n",
    "PPV = rf_ct.iloc[1][1]/rf_ct.iloc[1][2]\n",
    "NPV = rf_ct.iloc[0][0]/rf_ct.iloc[0][2]\n",
    "ACC = (rf_ct.iloc[0][0] + rf_ct.iloc[1][1]) / rf_ct.iloc[2][2]\n",
    "print(\"Random Forest: Sensitivity: %.2f Specificity: %.2f PPV: %.2f NPV: %.2f Accuracy: %.2f\" % (Sens, Spec, PPV, NPV, ACC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure\n",
    "\n",
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(best[0], best[1], pos_label=100)\n",
    "roc_auc = auc(rf_fpr, rf_tpr)\n",
    "plt.rcdefaults()\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(rf_fpr, rf_tpr, label='RNN')\n",
    "plt.xlabel('False positive rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "print(\"AUC = \", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_fpr)\n",
    "print(rf_tpr)\n",
    "print(rf_thresholds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
